{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-telegram-bot in /var/mobile/Containers/Data/Application/4A03FC81-5A84-4A45-818B-1BEA62D11CD3/Library/lib/python3.11/site-packages (21.1)\n",
      "Requirement already satisfied: httpx~=0.27 in /var/mobile/Containers/Data/Application/4A03FC81-5A84-4A45-818B-1BEA62D11CD3/Library/lib/python3.11/site-packages (from python-telegram-bot) (0.27.0)\n",
      "Requirement already satisfied: anyio in /private/var/containers/Bundle/Application/681A7ABF-B1B8-4972-915D-46945C945629/Carnets-sci.app/Library/lib/python3.11/site-packages (from httpx~=0.27->python-telegram-bot) (4.2.0)\n",
      "Requirement already satisfied: certifi in /private/var/containers/Bundle/Application/681A7ABF-B1B8-4972-915D-46945C945629/Carnets-sci.app/Library/lib/python3.11/site-packages (from httpx~=0.27->python-telegram-bot) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /var/mobile/Containers/Data/Application/4A03FC81-5A84-4A45-818B-1BEA62D11CD3/Library/lib/python3.11/site-packages (from httpx~=0.27->python-telegram-bot) (1.0.5)\n",
      "Requirement already satisfied: idna in /private/var/containers/Bundle/Application/681A7ABF-B1B8-4972-915D-46945C945629/Carnets-sci.app/Library/lib/python3.11/site-packages (from httpx~=0.27->python-telegram-bot) (3.6)\n",
      "Requirement already satisfied: sniffio in /private/var/containers/Bundle/Application/681A7ABF-B1B8-4972-915D-46945C945629/Carnets-sci.app/Library/lib/python3.11/site-packages (from httpx~=0.27->python-telegram-bot) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /var/mobile/Containers/Data/Application/4A03FC81-5A84-4A45-818B-1BEA62D11CD3/Library/lib/python3.11/site-packages (from httpcore==1.*->httpx~=0.27->python-telegram-bot) (0.14.0)\n",
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: dextools-python in /var/mobile/Containers/Data/Application/4A03FC81-5A84-4A45-818B-1BEA62D11CD3/Library/lib/python3.11/site-packages (0.3.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.3 in /var/mobile/Containers/Data/Application/4A03FC81-5A84-4A45-818B-1BEA62D11CD3/Library/lib/python3.11/site-packages (from dextools-python) (3.9.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /private/var/containers/Bundle/Application/681A7ABF-B1B8-4972-915D-46945C945629/Carnets-sci.app/Library/lib/python3.11/site-packages (from dextools-python) (2.31.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /var/mobile/Containers/Data/Application/4A03FC81-5A84-4A45-818B-1BEA62D11CD3/Library/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->dextools-python) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /private/var/containers/Bundle/Application/681A7ABF-B1B8-4972-915D-46945C945629/Carnets-sci.app/Library/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->dextools-python) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /var/mobile/Containers/Data/Application/4A03FC81-5A84-4A45-818B-1BEA62D11CD3/Library/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->dextools-python) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /var/mobile/Containers/Data/Application/4A03FC81-5A84-4A45-818B-1BEA62D11CD3/Library/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->dextools-python) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /var/mobile/Containers/Data/Application/4A03FC81-5A84-4A45-818B-1BEA62D11CD3/Library/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.3->dextools-python) (1.9.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /private/var/containers/Bundle/Application/681A7ABF-B1B8-4972-915D-46945C945629/Carnets-sci.app/Library/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->dextools-python) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /private/var/containers/Bundle/Application/681A7ABF-B1B8-4972-915D-46945C945629/Carnets-sci.app/Library/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->dextools-python) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /private/var/containers/Bundle/Application/681A7ABF-B1B8-4972-915D-46945C945629/Carnets-sci.app/Library/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->dextools-python) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /private/var/containers/Bundle/Application/681A7ABF-B1B8-4972-915D-46945C945629/Carnets-sci.app/Library/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->dextools-python) (2023.11.17)\n",
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\r"
     ]
    }
   ],
   "source": [
    "!pip install python-telegram-bot --upgrade\n",
    "!pip install dextools-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "api_key = \"b5ZO15hnCV9dnJr7XOQZIaGMWXCb6y5J7bXSoJvS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dextools_python import DextoolsAPIV2\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "dextools = DextoolsAPIV2(api_key, plan=\"trial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_liquidity_pools_sorted_by_date_df(chain='solana', start_date=None, end_date=None, num_pages=3):\n",
    "    all_tokens = []\n",
    "    for page in range(0, num_pages):\n",
    "        response = dextools.get_pools(chain, from_=end_date, to=start_date, page=page, order=\"desc\")\n",
    "        all_tokens.extend(response[\"data\"]['results'])\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # Flatten the nested 'socialInfo' and convert data to DataFrame\n",
    "    tokens_df = pd.json_normalize(all_tokens)\n",
    "    \n",
    "    return tokens_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_get_tokens():\n",
    "    tokens = get_liquidity_pools_sorted_by_date_df()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_rug_check_data(token_address):\n",
    "    # Base API URL\n",
    "    base_url = 'https://api.rugcheck.xyz/v1/tokens/'\n",
    "    \n",
    "    # Complete API URL with the token address\n",
    "    api_url = f'{base_url}{token_address}/report'\n",
    "    # Send GET request\n",
    "    response = requests.get(api_url)\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response into a pandas DataFrame\n",
    "        data = response.json()\n",
    "        df_top_holders = pd.json_normalize(data)\n",
    "    else:\n",
    "        good_resp = False\n",
    "        # Send GET request\n",
    "        while not good_resp:\n",
    "            time.sleep(2)\n",
    "            response = requests.get(api_url)\n",
    "            # Check if the request was successful\n",
    "            if response.status_code == 200:\n",
    "                # Parse the JSON response into a pandas DataFrame\n",
    "                data = response.json()\n",
    "                df_top_holders = pd.json_normalize(data)\n",
    "                good_resp = True\n",
    "    return df_top_holders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_top_10_holders_percentage(df):\n",
    "    # Assuming df['topHolders'] is a string representation of the topHolders list\n",
    "    # Convert the string to a list of dictionaries if needed\n",
    "    if isinstance(df['topHolders'].iloc[0], str):\n",
    "        df['topHolders'] = df['topHolders'].apply(eval)\n",
    "\n",
    "    # Normalize the nested JSON data for topHolders and calculate the sum of the 'pct' of the top 10 holders\n",
    "    top_holders_df = pd.json_normalize(df['topHolders'].iloc[0])\n",
    "    top_10_holders_pct = top_holders_df.nlargest(10, 'pct')['pct'].sum()\n",
    "\n",
    "    # Add the top 10 holders percentage to the DataFrame\n",
    "    df['top_10_holders_percentage'] = top_10_holders_pct\n",
    "\n",
    "    # Drop the original topHolders column if you want to remove it\n",
    "    #df.drop(columns=['topHolders'], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import json\n",
    "\n",
    "def calculate_liquidity(df):\n",
    "    # Convert string representation of lists back into actual lists for the 'topHolders' column\n",
    "    df['topHolders'] = df['topHolders'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    # Initialize total percentage and liquidity\n",
    "    total_pct = 0\n",
    "    liquidity_usd = 0\n",
    "\n",
    "    # Iterate over each row for topHolders to sum percentages and calculate liquidity\n",
    "    for index, row in df.iterrows():\n",
    "        # Calculate total percentage from topHolders\n",
    "        top_holders = row['topHolders']\n",
    "        total_pct += sum(holder['pct'] for holder in top_holders)\n",
    "        \n",
    "        # Calculate liquidity from markets (assuming 'markets' is a stringified JSON in the DataFrame)\n",
    "        markets_data = json.loads(row['markets']) if isinstance(row['markets'], str) else row['markets']\n",
    "        for market in markets_data:\n",
    "            liquidity_usd += market['lp']['baseUSD'] + market['lp']['quoteUSD']\n",
    "\n",
    "    \n",
    "    \n",
    "    # Add the calculated values as new columns to the DataFrame\n",
    "    df['total_percentage_of_top_holders'] = total_pct\n",
    "    df['total_liquidity_usd'] = liquidity_usd\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_each_top_10_holders_percentage(df):\n",
    "    # Assuming the 'topHolders' column is a list of dictionaries with the 'pct' key\n",
    "    for index, row in df.iterrows():\n",
    "        # Sort the top holders by percentage in descending order\n",
    "        sorted_holders = sorted(row['topHolders'], key=lambda x: x['pct'], reverse=True)\n",
    "        # Get the top ten holders\n",
    "        top_ten_holders = sorted_holders[:10]\n",
    "        for i, holder in enumerate(top_ten_holders):\n",
    "            # Create a new column for each top holder's percentage\n",
    "            df.at[index, f'top_holder_{i+1}_pct'] = holder['pct']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "def calculate_market_cap(df):\n",
    "    # Initialize an empty list to store market cap results\n",
    "    market_caps = []\n",
    "    \n",
    "    # Loop through each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # Check if markets is a string and convert it to a list if necessary\n",
    "        if isinstance(row['markets'], str):\n",
    "            markets = ast.literal_eval(row['markets'])\n",
    "        else:\n",
    "            markets = row['markets']\n",
    "        \n",
    "        # Initialize total market cap for the row\n",
    "        total_market_cap = 0\n",
    "\n",
    "        # Loop through each market to calculate its market cap\n",
    "        for market in markets:\n",
    "            base_price = market['lp']['basePrice']  # Extract the base price for the market\n",
    "            # Calculate market cap using total supply and base price\n",
    "            market_cap = row['token.supply'] / (10 ** row['token.decimals']) * base_price  \n",
    "            total_market_cap += market_cap  # Sum up the market cap for the row\n",
    "\n",
    "        # Append the calculated market cap for the row to the list\n",
    "        market_caps.append(total_market_cap)\n",
    "    \n",
    "    # Add the market cap list as a new column to the DataFrame\n",
    "    df['market_cap'] = market_caps\n",
    "    \n",
    "    # Return the updated DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_social_info(df):\n",
    "    # Create new columns with empty strings for each social info key\n",
    "    social_keys = ['discord', 'facebook', 'github', 'instagram', 'linkedin', \n",
    "                   'medium', 'reddit', 'telegram', 'tiktok', 'twitter', \n",
    "                   'website', 'youtube', 'bitbucket']\n",
    "    for key in social_keys:\n",
    "        df[key] = None\n",
    "\n",
    "    # Iterate over the DataFrame and update with social info\n",
    "    for index, row in df.iterrows():\n",
    "        token_data = dextools.get_token('solana', row['mint'])\n",
    "        social_info = token_data['data']['socialInfo']\n",
    "        for key in social_keys:\n",
    "            if key in social_info:\n",
    "                df.at[index, key] = social_info[key]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lp_locked_pct_from_df(df):\n",
    "    # Initialize a list to store the locked percentage values for each row\n",
    "    lp_locked_pcts = []\n",
    "    \n",
    "    # Loop through each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        if isinstance(row['markets'], str):\n",
    "            markets = ast.literal_eval(row['markets'])\n",
    "        else:\n",
    "            markets = row['markets']\n",
    "        # Convert the string representation of the list back to a list\n",
    "        try:\n",
    "            # Initialize a temporary variable to store the LP Locked Percentage\n",
    "            lp_locked_pct = 0\n",
    "            # Loop through the markets to find the 'lpLockedPct' value\n",
    "            for market in markets:\n",
    "                if 'lp' in market and 'lpLockedPct' in market['lp']:\n",
    "                    lp_locked_pct = market['lp']['lpLockedPct']\n",
    "                    break\n",
    "            # Append the percentage to the list\n",
    "            lp_locked_pcts.append(lp_locked_pct)\n",
    "        except ValueError:\n",
    "            # If there is an issue with converting the string to a list, append a default value\n",
    "            lp_locked_pcts.append(None)\n",
    "    \n",
    "    # Add the list as a new column to the DataFrame\n",
    "    df['lpLockedPct'] = lp_locked_pcts\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mc_prices(df, base):\n",
    "    # Create new columns for the USD and base currency prices\n",
    "    df['price_usd'] = 0.0\n",
    "    df['price_base'] = 0.0\n",
    "    \n",
    "    # Iterate over the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        if isinstance(row['markets'], str):\n",
    "            markets = ast.literal_eval(row['markets'])\n",
    "        else:\n",
    "            markets = row['markets']\n",
    "        \n",
    "        # If there are markets available, get the first one\n",
    "        if markets:\n",
    "            market = markets[0]\n",
    "            lp_info = market.get('lp', {})\n",
    "            \n",
    "            # Extract the base price in the base currency\n",
    "            base_price = lp_info.get('basePrice', 0)\n",
    "            # Extract the base price in USD\n",
    "            base_usd = lp_info.get('baseUSD', 0)\n",
    "            \n",
    "            # Update the DataFrame with the new information\n",
    "            df.at[index, 'price_usd'] = base_usd\n",
    "            df.at[index, f'price_base_{base}'] = base_price\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_a_sniper_liquidity_pool(liquidity_info_df, top_10_holders_pct_thrshld, total_liquidity_usd_thrshld, top_holder_1_pct_thrshld, lpLockedPct_thrshld):\n",
    "    # if top 10 holders hold greater than the thrshld\n",
    "    liquidity_info_df = liquidity_info_df.loc[liquidity_info_df['top_10_holders_percentage'] >= top_10_holders_pct_thrshld]\n",
    "    # if total liquidity in usd is less than the thrshld\n",
    "    liquidity_info_df = liquidity_info_df.loc[liquidity_info_df['total_liquidity_usd'] <= total_liquidity_usd_thrshld]\n",
    "    # if top holder holds greater than the thrshld\n",
    "    liquidity_info_df = liquidity_info_df.loc[liquidity_info_df['top_holder_1_pct'] >= top_holder_1_pct_thrshld]\n",
    "    # if the liquidity locked percentage is less than the thrshld\n",
    "    liquidity_info_df = liquidity_info_df.loc[liquidity_info_df['lpLockedPct'] <= lpLockedPct_thrshld]\n",
    "    # if does not have a value in the social media columns\n",
    "    social_keys = ['discord', 'facebook', 'github', 'instagram', 'linkedin', \n",
    "                   'medium', 'reddit', 'telegram', 'tiktok', 'twitter', 'youtube', 'bitbucket']\n",
    "    liquidity_info_df = liquidity_info_df.dropna(subset=social_keys)\n",
    "    liquidity_info_df = liquidity_info_df.dropna(subset=[\"website\"])\n",
    "    return liquidity_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# run the scraper on the entire set of addresses\n",
    "def get_liquidity_pools_signals(chain, base, num_lp, time_offset=0, start_date=None, end_date=None):\n",
    "    # calculate how many pages to grab from the API by getting the chain info\n",
    "    # and using the pageSize with num_lp to figure out how many pages to grab from the following API call\n",
    "    if not start_date:\n",
    "        start_date = datetime.datetime.now()\n",
    "    if not end_date:\n",
    "        end_date = start_date - datetime.timedelta(minutes=15)\n",
    "    \n",
    "    start_date_str = start_date.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    end_date_str = end_date.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    \n",
    "    response = response = dextools.get_pools(chain, from_=end_date_str, to=start_date_str, order=\"desc\")\n",
    "    page_size = response[\"data\"][\"pageSize\"]\n",
    "    total_pages = response[\"data\"][\"totalPages\"]\n",
    "    # calculate number of pages to pass into the next function\n",
    "    num_pages_to_scrape = math.ceil(num_lp / page_size)\n",
    "    if num_pages_to_scrape > total_pages:\n",
    "        print(\"Error: Too many liquidity pools requested for the timeframe!\")\n",
    "        return False\n",
    "    print(f'Total number of pages: {total_pages}\\nTotal number of LPs per page: {page_size}')\n",
    "    print(f'Total number of LPs to scrape: {num_lp}\\nTotal number of pages to scrape: {num_pages_to_scrape}')\n",
    "    # Update message\n",
    "    print(\"Grabbing the liquidity pools 游뱀\")\n",
    "    # set a timeout to satisify num request per sec\n",
    "    time.sleep(2)\n",
    "    # Get recently launched Liquidiy Pools and extract the token addresses\n",
    "    lp_df = get_liquidity_pools_sorted_by_date_df(chain, start_date=start_date, end_date=end_date, num_pages=num_pages_to_scrape)\n",
    "    mc_addr_lst = lp_df[\"mainToken.address\"].tolist()\n",
    "    # update message\n",
    "    print(\"COMPLETED 游땙\")\n",
    "    print(\"Starting Rug Checker for each address 游댧\\n\\n\")\n",
    "    # rest before the rug checking\n",
    "    time.sleep(2)\n",
    "    # filter the list based off the startegy\n",
    "    filter_mc_addr_lst = []\n",
    "    # for each address in mc_addr_lst get the rugchecker info, social media info, and run the analysis.\n",
    "    # if coin passes the analysis, pass the address to filter_mc_addr_lst\n",
    "    for coin_addr in mc_addr_lst[:num_lp]:\n",
    "        print(f'Running Rug Checker for coin with addr == {coin_addr} 游륋릞')\n",
    "        # get rugchecker info\n",
    "        rug_check_df = get_rug_check_data(coin_addr)\n",
    "        # Add the top 10 holders percentage of coins\n",
    "        rug_check_df = add_top_10_holders_percentage(rug_check_df)\n",
    "        # Add each of the 10 top holders percentages to the df\n",
    "        rug_check_df = add_each_top_10_holders_percentage(rug_check_df)\n",
    "        # Calculate and add the liquidity in USD\n",
    "        rug_check_df = calculate_liquidity(rug_check_df)\n",
    "        # Calaculate and add the Liquidity Locked Percentage\n",
    "        rug_check_df = calculate_lp_locked_pct_from_df(rug_check_df)\n",
    "        # Calculate and add the market capitalization\n",
    "        rug_check_df = calculate_market_cap(rug_check_df)\n",
    "        # Calculate the price of the meme coin in USD and the base coin\n",
    "        rug_check_df = calculate_mc_prices(rug_check_df, base)\n",
    "        # Add the social info\n",
    "        rug_check_df = add_social_info(rug_check_df)\n",
    "        # check if we can snipe this LP\n",
    "        rug_check_df = is_a_sniper_liquidity_pool(rug_check_df, 50, 1000, 5, 70)\n",
    "        if rug_check_df.shape[0] == 0:\n",
    "            print(f'游녩游쮫릠뀛릞쮫릠뀛릞 DROPPED coin with addr == {coin_addr} 游녩游쮫릠뀛릞쮫릠뀛릞쬪\n')\n",
    "        else:\n",
    "            print(f'游릭游릭游릭 SNIPE SIGNAL FOR COIN WITH ADDR == {coin_addr} 游릭游릭游릭\\n')\n",
    "            filter_mc_addr_lst.append({\n",
    "                \"coin_addr\": coin_addr,\n",
    "                \"rug_check_info\": rug_check_df.head(1).to_json(orient=\"records\")\n",
    "            })\n",
    "        # rest after each iteration\n",
    "        time.sleep(2)\n",
    "    print(filter_mc_addr_lst)\n",
    "    return True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pages: 2\n",
      "Total number of LPs per page: 20\n",
      "Total number of LPs to scrape: 10\n",
      "Total number of pages to scrape: 1\n",
      "Grabbing the liquidity pools 游뱀\n",
      "COMPLETED 游땙\n",
      "Starting Rug Checker for each address 游댧\n",
      "\n",
      "\n",
      "Running Rug Checker for coin with addr == 7MUKpncTCa7BbHEF2ouSzJFtyBFpX8JooVzAbhXiymCh 游륋릞쬪n",
      "游녩游쮫릠뀛릞쮫릠뀛릞 DROPPED coin with addr == 7MUKpncTCa7BbHEF2ouSzJFtyBFpX8JooVzAbhXiymCh 游녩游쮫릠뀛릞쮫릠뀛릞쬪n",
      "\n",
      "Running Rug Checker for coin with addr == 44nziFXpnw5fEsML4g3pzpkHSKid5BiirZwevBP8XXth 游륋릞쬪n",
      "游릭游릭游릭 SNIPE SIGNAL FOR COIN WITH ADDR == 44nziFXpnw5fEsML4g3pzpkHSKid5BiirZwevBP8XXth 游릭游릭游릭\n",
      "\n",
      "Running Rug Checker for coin with addr == GfJHKwLnHprhPBpnuVxaN7Jt6C4ZYZEeZMs94dDEzqyb 游륋릞쬪n",
      "游녩游쮫릠뀛릞쮫릠뀛릞 DROPPED coin with addr == GfJHKwLnHprhPBpnuVxaN7Jt6C4ZYZEeZMs94dDEzqyb 游녩游쮫릠뀛릞쮫릠뀛릞쬪n",
      "\n",
      "Running Rug Checker for coin with addr == DyugVhjgSgSTbGqmmMrgwPQF5uA8y8VX2wCo42hJRGVE 游륋릞쬪n",
      "游녩游쮫릠뀛릞쮫릠뀛릞 DROPPED coin with addr == DyugVhjgSgSTbGqmmMrgwPQF5uA8y8VX2wCo42hJRGVE 游녩游쮫릠뀛릞쮫릠뀛릞쬪n",
      "\n",
      "Running Rug Checker for coin with addr == Ei3WAHK3EkJPZjfScv9g8XGp51CdQh7V8EmbKw6GaDPb 游륋릞쬪n",
      "游녩游쮫릠뀛릞쮫릠뀛릞 DROPPED coin with addr == Ei3WAHK3EkJPZjfScv9g8XGp51CdQh7V8EmbKw6GaDPb 游녩游쮫릠뀛릞쮫릠뀛릞쬪n",
      "\n",
      "Running Rug Checker for coin with addr == 2EEoXEBxYFSeYQe6HP1LMimi5Rb6Lbu5zF5SSEpZmegD 游륋릞쬪n",
      "游녩游쮫릠뀛릞쮫릠뀛릞 DROPPED coin with addr == 2EEoXEBxYFSeYQe6HP1LMimi5Rb6Lbu5zF5SSEpZmegD 游녩游쮫릠뀛릞쮫릠뀛릞쬪n",
      "\n",
      "Running Rug Checker for coin with addr == FKPTPg6mgARnPrB3bYh5qwd9qEsZdLsRt4DbbBTo17XB 游륋릞쬪n",
      "游녩游쮫릠뀛릞쮫릠뀛릞 DROPPED coin with addr == FKPTPg6mgARnPrB3bYh5qwd9qEsZdLsRt4DbbBTo17XB 游녩游쮫릠뀛릞쮫릠뀛릞쬪n",
      "\n",
      "Running Rug Checker for coin with addr == 9JzpwWAbXxE8pH9fqpCBfuXyguAfjGJiimFcDsCmX1g4 游륋릞쬪n",
      "游녩游쮫릠뀛릞쮫릠뀛릞 DROPPED coin with addr == 9JzpwWAbXxE8pH9fqpCBfuXyguAfjGJiimFcDsCmX1g4 游녩游쮫릠뀛릞쮫릠뀛릞쬪n",
      "\n",
      "Running Rug Checker for coin with addr == BwThyrfPagunf3sswhYmvxZYM5CGAhfS4CvyQJWyR3f8 游륋릞쬪n",
      "游녩游쮫릠뀛릞쮫릠뀛릞 DROPPED coin with addr == BwThyrfPagunf3sswhYmvxZYM5CGAhfS4CvyQJWyR3f8 游녩游쮫릠뀛릞쮫릠뀛릞쬪n",
      "\n",
      "Running Rug Checker for coin with addr == An5kZ7bMhMtwNcEAGyg4hgZyqEfq9bi89Ap1wSnJjFgh 游륋릞쬪n",
      "游녩游쮫릠뀛릞쮫릠뀛릞 DROPPED coin with addr == An5kZ7bMhMtwNcEAGyg4hgZyqEfq9bi89Ap1wSnJjFgh 游녩游쮫릠뀛릞쮫릠뀛릞쬪n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_liquidity_pools_signals(\"solana\", \"SOL\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
